# Qwen2.5-Omni Technical Report

Qwen2.5-Omni 是一种端到端的多模态模型，能够处理文本、图像、音频和视频，并同时以流式方式同时生成文本和自然语音响应。

为了将视频输入的时间戳与音频同步，以交错的方式顺序组织音频和视频，并提出了一种新的位置嵌入方法，称为 TMRoPE（Time-aligned Multimodal RoPE）。
为了同时生成文本和语音，同时避免两种模式之间的干扰，提出了 Thinker-Talker 架构。
在这个框架中，Thinker 充当负责文本生成的大型语言模型，而 Talker 是一个双轨自回归模型，它直接利用 Thinker 中的隐藏表示来生成音频标记作为输出。
Thinker 和 Talker 模型都旨在以端到端的方式进行训练和推断。

为了以流式方式解码音频标记，我们引入了一个滑动窗口 DiT，它限制了感受野，旨在减少初始包延迟。


unified omni-model的开发需要考虑以下几个关键因素：
1. 实现一种系统的方法来联合训练各种模式，包括文本、图像、视频和音频，以促进它们之间的相互增强是至关重要的。这种对齐对于视频内容尤其重要，其中有必要同步音频和视觉信号的时间方面。
2. 必须管理不同模式的输出之间的潜在干扰，确保文本和语音标记等输出的训练过程不会相互破坏。
3. 需要探索能够实时理解多模态信息并允许有效音频输出流的架构设计，从而减少初始延迟。

为了解决第一个挑战，我们提出了一种新的位置嵌入方法，称为 TMRoPE（时间对齐的多模态 RoPE）。我们以交错的结构组织这些音频和视频帧，以按时间顺序表示视频序列。
对于第二个挑战，我们提出了 Thinker-Talker 架构，其中 Thinker 的任务是文本生成，而 Talker 专注于生成流式语音标记。Talker 直接从 Thinker 接收高级表示。这种设计的灵感来自于人类利用不同的器官来产生各种信号的方式，这些信号同时通过相同的神经网络协调
为了解决与流相关的挑战，并促进多模态信号实时理解所需的预填充，我们建议通过采用块流处理方法对所有多模态编码器进行修改。为了支持流语音生成，我们实现了一个双轨自回归模型，该模型生成语音标记，以及一个将这些标记转换为波形的 DiT 模型，从而实现流音频生成和最小化初始延迟

