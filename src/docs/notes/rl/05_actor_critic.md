# Actor-Critic

基于值函数的方法只学习一个价值函数，基于策略的方法只学习一个策略函数，本文提到的算法既学习价值函数，又学习策略函数。

::: tip
Actor-Critic 算法本质上是基于策略的算法，其目标是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。
:::

在上一节 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。 REINFOCE 算法用蒙特卡洛方法来估计 $Q(s,a)$，
Actor-Critic 算法使用拟合一个值函数来指导策略进行学习。在策略梯度中，可以把梯度写成下面这个更加一般的形式